{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosas por revisar\n",
    "\n",
    "- Cuando se aplica el zoom no se hace en la posicion que tiene el mouse sobre la imagen zoomeada, sino sobra la imagen original, eso quiere decir que so hago click sobre la derecha primero se va a zoomear correctamente, pero si despues hago click, en la imagen con un primer zoom, sobre la izquierda (que sería el centro de la imagen original) se va a ir a un zoom mas pero sobre la izquierda de la imagen original, no de la zoomeada (que, de nuevo, sería el centro de la imagen original). Hacer la prueba si no se entiende. Pienso que una solucion puede ser que cuando se aplique el zoom se guarden las coordenadas de los extremos donde se hizo zoom en una variable current_position y que la posicion x e y del mouse pueda referenciarse a esa current_position, pero hoy que hacer bien la matematica porque necesita un escalamiento, asi rapido creo que seria por ejemplo: new_left = current_position[0] + x/zoom_rate - width/zoom_rate*2\n",
    "\n",
    "- Agregar el historial de posiciones para aplicar IA y aplicar gestos. Tener en cuenta que se puede submuestrear los datos para no recargar al sistema, tal vez 1 o 2 posiciones por segundo alcanza\n",
    "\n",
    "- Utilizar en conjunto con el mouse, porque solo se probaron los cambios de imagenes. Ver si hace falta mejorar la velocidad (por ejemplo, se puede agregar un condicional que solo transforme a la imagen si hubo un click, porque ahora el cv2.warpAfine se esta ejecutando todas las veces, y es una multiplicacion de matrices..)\n",
    "\n",
    "- Revisar resolucion de la ventana. Otra cosa: el flip se puede sacar y usar el war affine haciendo que left se haga right y right se haga left (digamos que simplemente hay que cambiar las coordenadas en init_position. Tambien habría que ver como dejar todo listo para poder abrir la ventana para claibrar el zoom y los parametros de sensibilidad, etc. y después poder cerrar la ventana y que el tracking siga funcionando.\n",
    "\n",
    "- Threshold adaptativo: ver si conviene o no.\n",
    "\n",
    "- Mejorar el temblor del puntero viendo si se puede hacer que el threshold de movimiento dependa de la escala de cada movimiento, es decir, si hago un movimiento MUY grande en x  y uno MUY pequeño en y, que solo genere el movimiento en x."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import IPython.display\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import mouse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zoom Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to be called on mouse callback\n",
    "def mousecall(event,x,y,flags,param):\n",
    "    global zoom_rate\n",
    "    global M\n",
    "    \n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        zoom_rate += 1\n",
    "        M = apply_zoom(x,y, zoom_rate, False)\n",
    "    elif event == cv2.EVENT_RBUTTONDOWN:\n",
    "        zoom_rate = max(1, zoom_rate-1)\n",
    "        M = apply_zoom(x,y, zoom_rate, True)\n",
    "\n",
    "#I think this function is used to \"detect\" the mouse button release, it was used like this in the example I used as guide\n",
    "def mousenone(event,x,y,flags,param):\n",
    "\n",
    "    return 0\n",
    "\n",
    "#This function gets the Transformation Matrix to apply to the image\n",
    "def apply_zoom(x, y, zoom_rate, is_zoom_out):\n",
    "    global width\n",
    "    global height\n",
    "    \n",
    "    cv2.setMouseCallback('frame', mousenone)\n",
    "    \n",
    "    new_left = x-width/(zoom_rate*2)\n",
    "    new_right = x+width/(zoom_rate*2)\n",
    "    new_down = y+height/(zoom_rate*2) #take into consideration that the image is drawn from the upper left, so up is diff and down is sum\n",
    "    new_up = y-height/(zoom_rate*2)\n",
    "\n",
    "    new_position = np.float32([[new_left,new_up],[new_right,new_up],[new_left,new_down],[new_right,new_down]])\n",
    "    \n",
    "    matrix = cv2.getPerspectiveTransform(new_position,init_position)\n",
    "    \n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alarm Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_alarms(frame, image_returned, mouse_coords):\n",
    "    check_source(image_returned)\n",
    "    check_reflex(frame)\n",
    "    check_centroid_colour(frame, mouse_coords)\n",
    "    \n",
    "def raise_alarms():\n",
    "    global source_flag\n",
    "      \n",
    "    if reflex_flag_counter > 40:\n",
    "        print(\"Can't detect reflex point\")\n",
    "    if source_flag:\n",
    "        print(\"Can't get source of image\")\n",
    "    if bright_flag_counter > 40:\n",
    "        print(\"Too much bright in the background\")   \n",
    "    if centroid_surroundings_flag_counter> 40:\n",
    "        print(\"There are too many reflective stickers\")  \n",
    "    \n",
    "def check_reflex(frame): #sobre este check ahora probamos tanto el reflex como el brillo de fondo\n",
    "    global reflex_flag_counter\n",
    "    global bright_flag_counter\n",
    "    \n",
    "    colour_values, colour_counts = np.unique(frame, return_counts = True)\n",
    "    colour_counts_total = np.sum(colour_counts)  \n",
    "    \n",
    "    for i in range(colour_values.shape[0]):\n",
    "        if colour_values[i] == 0:\n",
    "            if colour_counts[i]/colour_counts_total > 0.99:\n",
    "                reflex_flag_counter += 1\n",
    "            else:\n",
    "                reflex_flag_counter = 0\n",
    "        \n",
    "        else:\n",
    "            if colour_counts[i]/colour_counts_total > 0.01:  #División de los blancos sobre el total de pixels.\n",
    "                bright_flag_counter += 1     \n",
    "            else:                                            #Elegí 1% porque es el complemento del 99% definido en reflex\n",
    "                bright_flag_counter = 0\n",
    "         \n",
    "        \n",
    "def check_source(image_returned):\n",
    "    global source_flag\n",
    "    \n",
    "    source_flag = not image_returned\n",
    "    \n",
    "    \n",
    "def check_centroid_colour(frame, mouse_coords):       #falta agregarle la parte de deteccion de contornos   \n",
    "    global centroid_surroundings_flag_counter\n",
    "    \n",
    "    \n",
    "#    print(\"estoy en check_centroid_colour y el centroide es color: \", pointer_colour)\n",
    "    \n",
    "    cX=mouse_coord[0]\n",
    "    cY=mouse_coord[1]\n",
    "     \n",
    "    recorte = frame[max(cY-20, 0): min(cY+20, height), max(cX-20, 0): min(cX+20, width)]\n",
    "\n",
    "    colour_values, colour_counts = np.unique(recorte, return_counts = True)    \n",
    "    colour_counts_total = np.sum(colour_counts)   \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    print(colour_values , colour_counts)              #Prueba: borrar cuando terminemos de hacer la función\n",
    "#    plt.imshow(recorte)                               #Prueba: borrar cuando terminemos de hacer la función\n",
    "#    plt.show()                                        #Prueba: borrar cuando terminemos de hacer la función\n",
    "\n",
    "                \n",
    "                \n",
    "                \n",
    "    if colour_values[0] == 0:                                   # calculo el porcentaje de negros en la\n",
    "        blacks_percentage = colour_counts[0]/colour_counts_total     # imagen recortada.\n",
    "    else:\n",
    "        blacks_percentage = 0 # será 0 en caso de que el subíndice 0 de color values sea un 1, porque no existen\n",
    "                              # puntos negros\n",
    "        \n",
    "\n",
    "#    print(\"el porcentaje de negros es: \", blacks_percentage)  #veo el porcentaje de píxeles negros.\n",
    " \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    if pointer_colour == 0 and ( 0.7 < blacks_percentage or blacks_percentage < 0.35):\n",
    "\n",
    "        centroid_surroundings_flag_counter += 1\n",
    "    else:\n",
    " \n",
    "        centroid_surroundings_flag_counter = 0\n",
    "        \n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "  \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function gets the frame from the camera and apply the transformations, also applies the movement to the mouse\n",
    "def get_frame(cap, mouse_coord):\n",
    "    global alarm_check_starttime\n",
    "    \n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    print(frame.shape)\n",
    "    if ret:\n",
    "        frame = cv2.warpPerspective(frame, M, (int(width), int(height)))\n",
    "\n",
    "        frame = cv2.flip(frame, 1)\n",
    "\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) #CV works with BGR instead of RGB (its weird I know), we switch to it and then back for it to be easy to work with\n",
    "\n",
    "        frame = color_extract(frame)\n",
    "        \n",
    "        mouse_coord = mouse_movement(detect_centroid(frame), mouse_coord)\n",
    "    \n",
    "        #frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR) #back to BGR\n",
    "    \n",
    "    check_alarms(frame, ret, mouse_coord)\n",
    "    raise_alarms()\n",
    "    \n",
    "    return frame, mouse_coord\n",
    "\n",
    "#This function creates a new 1D array that has 255 where all the pixel values on the three colour depths lie bewtween each\n",
    "#limit in lower and upper, and 0 otherwise (so a pixel will be 255 if lower < pixel value < upper on ALL colour matrix)\n",
    "def color_extract(image):\n",
    "    \n",
    "    #Masking 255,144,104\n",
    "    lower = np.array([230,230,230])\n",
    "    upper = np.array([255,255,255])\n",
    "    \n",
    "    res = cv2.inRange(image, lower, upper)\n",
    "    \n",
    "    return res\n",
    "\n",
    "#This function detects the centroid position\n",
    "#Detect centroid\n",
    "def detect_centroid(res):\n",
    "    global pointer_colour\n",
    "    M = cv2.moments(res)\n",
    "    if M[\"m00\"] != 0:\n",
    "        cX = int(M[\"m10\"] / M[\"m00\"])\n",
    "        cY = int(M[\"m01\"] / M[\"m00\"])\n",
    "    else:\n",
    "        cX, cY = 0, 0\n",
    "    \n",
    "    pointer_colour = res[cY, cX]                # almaceno el color del pixel sobre el que está el centroide\n",
    "    \n",
    "    print(\"el color sobre el cursor es: \", pointer_colour) \n",
    "    \n",
    "# Chequeo si el color del centroide es negro y el porcentaje de blancos es bajo\n",
    "# para activar una alarma\n",
    "\n",
    "#    if pointer_color == 0 and blacks_percentage > 0.5:  # considerando que el objeto reflectante tiene aproximadamente\n",
    "                                                         # 500-1000 píxeles de tamaño, tomo como valor arbitrario 800\n",
    "                                                         # píxeles de tamaño, lo que representa el 50% del recorte.\n",
    "                                                         # si la cantidad de negros es menor a ese porcentaje, alarma.\n",
    "        \n",
    "    \n",
    "    \n",
    "    cv2.rectangle(res, (cX - 1, cY - 1), (cX + 1, cY + 1), (255, 255, 255), -1)\n",
    "    cv2.putText(res, \"centroid\", (cX - 25, cY - 25),cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "    return [cX,cY]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mouse Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function moves the mouse to the new position\n",
    "def mouse_movement(newPos, oldPos):\n",
    "    \n",
    "    #Avoind jerking when the point is lost\n",
    "    if newPos[0]==0 and newPos[1]==0 :\n",
    "        newPos = oldPos\n",
    "    \n",
    "    #hist_points_cent.append(newPos)  #History of centroid movements for gestures\n",
    "    \n",
    "    #calculate Movement Delta\n",
    "    deltaX = (newPos[0]-oldPos[0])\n",
    "    deltaY = (newPos[1]-oldPos[1])\n",
    "    \n",
    "    #Aplicamos un delta, si el movimiento es menor a ese delta no se mueve.\n",
    "    if abs(deltaX)>delta_threshold or abs(deltaY)>delta_threshold:\n",
    "        #print(\"Moved\")\n",
    "        mouse.move(mouse_sensibility(deltaX,'x'), mouse_sensibility(deltaY,'y'), absolute=False, duration=0)\n",
    "    \n",
    "    return newPos\n",
    "\n",
    "def mouse_sensibility(delta, axis):\n",
    "    assert axis == \"x\" or axis == \"y\"\n",
    "    if abs(delta) > sensibility_threshold:\n",
    "        return delta*sensibility_dict[axis]\n",
    "    else:\n",
    "        return delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(480, 640, 3)\n",
      "el color sobre el cursor es:  255\n",
      "[255] [1600]\n",
      "Too much bright in the background\n"
     ]
    }
   ],
   "source": [
    "#Image Params\n",
    "height = 480\n",
    "width = 640\n",
    "zoom_rate = 1\n",
    "M = np.array([[1,0,0],[0,1,0],[0,0,1]], dtype = np.float32)\n",
    "\n",
    "init_position = np.float32([[0,0],[width,0],[0,height],[width,height]]) #Initial position for the corners of the image\n",
    "\n",
    "#Flags\n",
    "reflex_flag_counter = 0\n",
    "source_flag = False\n",
    "bright_flag_counter = 0\n",
    "centroid_surroundings_flag_counter = 0\n",
    "\n",
    "\n",
    "#h_center = width/2\n",
    "#v_center = height/2\n",
    "#left = h_center - width/(2*zoom_rate)\n",
    "#right = h_center + width/(2*zoom_rate)\n",
    "#upper = v_center - height/(2*zoom_rate)\n",
    "#lower = h_center + height/(2*zoom_rate)\n",
    "#zoom_position = np.float32([[left,upper],[right,upper],[left,lower],[right,lower]])\n",
    "#\n",
    "#M = cv2.getPerspectiveTransform(zoom_position,init_position)\n",
    "\n",
    "#Mouse Params\n",
    "mouse_coord = [1,1]\n",
    "delta_threshold = 1 #Threshold to interpret a centroid movement as a mouse movement\n",
    "\n",
    "smoothness = 5\n",
    "\n",
    "sensibility_dict = {\"x\" : 1,\n",
    "                    \"y\" : 1} #mouse sensibility\n",
    "sensibility_threshold = 10 #mouse sensibility threshold\n",
    "\n",
    "#Historical points for gestures detection\n",
    "#hist_points_cent = []\n",
    "#hist_points_mouse = []\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 10000)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 10001)\n",
    "    \n",
    "cv2.namedWindow('frame', cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow('frame', width, height)\n",
    "\n",
    "try:\n",
    "    while(True):\n",
    "\n",
    "        cv2.setMouseCallback('frame', mousecall)\n",
    "\n",
    "        frame, mouse_coord = get_frame(cap, mouse_coord)\n",
    "\n",
    "        if not source_flag:\n",
    "            cv2.imshow('frame',frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "        elif cv2.waitKey(1) & 0xFF == ord('r'):\n",
    "            zoom_rate = 1\n",
    "            M = np.array([[1,0,0],[0,1,0],[0,0,1]], dtype = np.float32)\n",
    "\n",
    "        clear_output(wait=True)\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "except KeyboardInterrupt:    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Links utiles:\n",
    "\n",
    "- https://gist.github.com/nochekaiser/ab4b3b8b8ceaad7db7c0c303b0ff661d\n",
    "- https://docs.opencv.org/3.4/db/d5b/tutorial_py_mouse_handling.html\n",
    "- https://docs.opencv.org/3.4/da/d97/tutorial_threshold_inRange.html\n",
    "- https://docs.opencv.org/3.4/d7/dfc/group__highgui.html\n",
    "- https://docs.opencv.org/3.4/d2/de8/group__core__array.html\n",
    "- https://www.programcreek.com/python/example/70462/cv2.setMouseCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
